config:
    objective:
        name: LeNet_MNIST

    convex_conjugate:
        max_iter: 700
        gtol: 1e-1

    F_sampler:
        name: uniform_F_sampler_with_key

    architecture:
        neural_u:
            layers: [128, 128, 128, 128]
            act_fn: "elu"
            rank_pos_def: 1
            optimizer:
                name: "adam"
                scheduler:
                    name: cosine_decay_schedule
                    options:
                        init_value: 1e-3
                        alpha: 0.01
                        decay_steps: 10000
                b1: 0.5
                b2: 0.5

    
        neural_i:
            latent_embed_dim: 512
            nb_layers: 2
            act_fn: "silu"
            optimizer:
                name: "adam"
                scheduler:
                    name: cosine_decay_schedule
                    options:
                        init_value: 5e-4
                        alpha: 0.01
                        decay_steps: 50000
        
        neural_conj_u:
            layers: [512, 512]
            act_fn: "relu"
            NN: "MLP"
            optimizer:
                name: "adam"
                scheduler:
                    name: cosine_decay_schedule
                    options:
                        init_value: 5e-4
                        alpha: 0.01
                        decay_steps: 11000
                b1: 0.9
                b2: 0.999

    
    training_hyper:
        seed: 0
        batch_size_u: 128
        batch_size_i: 128
        num_iter_u: 10000
        num_iter_i: 50000
        num_warmup_conj_u: 1000
    
    training_fn:
        sigma_diff: 1.0
    
    saving:
        output_dir: "/data/workspace/nina/wandb"
        model_state_dir_u: "model_state_u"
        model_state_dir_conj_u: "model_state_conj_u"
        model_state_dir_i: "model_state_i"
    
    logging:
        project: "NPF_github"
        log_freq: 100
        log_freq_images: 1000
        offline: False    
