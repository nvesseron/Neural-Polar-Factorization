import jax
import jax.numpy as jnp
import functools


def get_train_step_conj_u(): 
    """Create loss and training function for the warm up of the conjugate NN."""
    def loss_fn_conj_u(params_conj_u, predict_conj_u, params_u, predict_u, input):
        """Loss function to train the conjugate to inverse the field \nabla u"""
        grad_u_point = jax.grad(predict_u, argnums=1)
        grad_u = jax.vmap(lambda x: grad_u_point({"params": params_u}, x))
        pred_grad_u = grad_u(input)
        input_pred = predict_conj_u({"params": params_conj_u}, pred_grad_u)
        loss_conj_u = jnp.mean((input_pred - input) ** 2)
        return loss_conj_u

    @jax.jit
    def train_step_conj_u(state_u, state_conj_u, input):
        """Step function"""
        value_and_grad_fn = jax.value_and_grad(loss_fn_conj_u)
        loss, grad = value_and_grad_fn(state_conj_u.params, state_conj_u.apply_fn, state_u.params, state_u.apply_fn, input)
        return state_conj_u.apply_gradients(grads=grad), {"loss conj u": loss}
    return train_step_conj_u


def get_train_step_dual_u(apply_grad_conj, conjugate_solver, max_iter):
    """Create loss and training function to train u as well as the conjugate network."""
        
    def loss_fn(params_u, params_conj, predict_u, predict_conj, batch):
        """Loss function"""
        source, target = batch["input"], batch["target"]
        # initialize the conjugate solver with the prediction of the conjugate NN
        grad_conj = jax.vmap(lambda x: apply_grad_conj(predict_conj)({"params": params_conj}, x))
        init_source_hat = grad_conj(target)

        #to avoid nan value in early iterations
        has_no_nan_init = ~jnp.isnan(jnp.sum(init_source_hat, axis=1))
        init_source_hat = jnp.nan_to_num(init_source_hat)
        
        # conjugate solveur initialized with predictions of V
        u_value_partial = lambda x: predict_u({"params": params_u}, x)
        def finetune_source_hat(y, x_init):
            state_solver = conjugate_solver.solve(
            u_value_partial, y, x_init=x_init)
            has_converged_ = 1 - (state_solver.num_iter > max_iter - 1)
            return state_solver.grad, has_converged_
        
        finetune_source_hat = jax.vmap(finetune_source_hat)
        source_hat_detach, has_converged_ = jax.lax.stop_gradient(
            finetune_source_hat(target, init_source_hat)
            )
        
        # to avoid nan generated by the conjugate and only optimize on converged input 
        has_no_nan = ~jnp.isnan(jnp.sum(source_hat_detach, axis=1))
        has_converged = has_no_nan * has_converged_
        source_hat_detach = jnp.nan_to_num(source_hat_detach)

        # compute loss
        batch_dot = jax.vmap(jnp.dot)
        u_source = u_value_partial(source)
        u_star_target = batch_dot(source_hat_detach,
                                target) - u_value_partial(source_hat_detach)
        dual_source = (u_source * has_converged).mean()
        dual_target = (u_star_target * has_converged).mean()
        dual_loss = dual_source + dual_target
        amor_loss = ((init_source_hat - source_hat_detach) ** 2).mean()
        loss = dual_loss + amor_loss
      
        return loss, (dual_loss, amor_loss, jnp.mean(has_converged_)*100, jnp.mean(has_no_nan)*100, jnp.mean(has_no_nan_init)*100)

    @jax.jit
    def step_fn(state_u, state_conj, batch):
        """Training step"""
        grad_fn = jax.value_and_grad(loss_fn, argnums=[0, 1], has_aux=True)
        # compute loss and gradients
        (loss, (loss_u, loss_conj, has_converged, has_no_nan, has_no_nan_init)), (grads_u, grads_conj) = grad_fn(
            state_u.params,
            state_conj.params,
            state_u.apply_fn,
            state_conj.apply_fn,
            batch,
            )
        log = {"loss u": loss, "loss_u": loss_u, "loss_conj": loss_conj, "has_converged": has_converged, "has_no_nan": has_no_nan, "has_no_nan_init": has_no_nan_init}
        # update state
        return (
            state_u.apply_gradients(grads=grads_u),
            state_conj.apply_gradients(grads=grads_conj), log
            )

    return step_fn


        
def get_step_fn_flow(sigma_diff: float):
    """Create loss and training function to train u as well as the conjugate network.

    Args:
        sigma_diff: standard deviation of the noise injected during the training of the flow
    """

    def loss_fn(params, predict, t, noise, batch):
        """Loss function"""
        x_t = (1-t) * batch["input"] + t * batch["target"] + sigma_diff * jnp.sqrt(( t * (1-t) )) * noise
        apply_fn = functools.partial(
            predict, {"params": params}
        )
        v_t = jax.vmap(apply_fn)(
            t=t, x=x_t, condition=batch["input"]
        )
        return jnp.mean((v_t - batch["target"]) ** 2)
    
    @jax.jit
    def step_fn(key, state_velocity_field, batch):
        """Training step"""
        (batch_size, dim_source) = batch["input"].shape
        key_noise, key_t = jax.random.split(key, 2)
        t = jax.random.uniform(key_t, (batch_size,1))
        noise = jax.random.normal(key_noise, (batch_size, dim_source))

        loss_grad_fn = jax.value_and_grad(loss_fn)
        loss, grads = loss_grad_fn(
            state_velocity_field.params, state_velocity_field.apply_fn, t, noise, batch
        )
        return state_velocity_field.apply_gradients(grads=grads), {"loss i": loss}
    return step_fn

def get_step_fn_regression():
    """Create loss and training function for learning M."""
    def loss_fn_reg(params_m, predict_m, batch):
        """Loss function"""
        pred_m = predict_m({"params": params_m}, batch["input"])
        loss = jnp.mean((pred_m - batch["target"]) ** 2)
        return loss

    @jax.jit
    def train_step_m(state_m, batch):
        """Step function"""
        value_and_grad_fn = jax.value_and_grad(loss_fn_reg)
        loss, grad = value_and_grad_fn(state_m.params, state_m.apply_fn, batch)
        return state_m.apply_gradients(grads=grad), {"loss M": loss}
    return train_step_m